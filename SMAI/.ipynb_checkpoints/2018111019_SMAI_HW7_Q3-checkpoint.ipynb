{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwxWcO7fqb-L"
   },
   "source": [
    "# Excercise - Multi-class classification of MNIST using Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcygblmOmQDZ"
   },
   "source": [
    "In binary perceptron, where $\\mathbf{y} \\in \\{-1, +1\\}$, we used to update our weights only for wrongly classified examples.\n",
    "\n",
    "The multi-class perceptron is regarded as a generalization of binary perceptron. Learning through iteration is the same as the perceptron. Weighted inputs are passed through a multiclass signum activation function. If the predicted output label is the same as true label then weights are not updated. However, when predicted output label $\\neq$ true label, then the wrongly classified input example is added to the weights of the correct label and subtracted from the weights of the incorrect label. Effectively, this amounts to ’rewarding’ the correct weight vector, ’punishing’ the misleading, incorrect weight\n",
    "vector, and leaving alone an other weight vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8p1aMxn8zer7"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set();\n",
    "import pandas as pd\n",
    "import math\n",
    "# import gif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gNkGLnbjTY-s"
   },
   "outputs": [],
   "source": [
    "# Setting the seed to ensure reproducibility of experiments\n",
    "np.random.seed(11)\n",
    "\n",
    "# One-hot encoding of target label, Y\n",
    "def one_hot(a):\n",
    "    b = -1 * np.ones((a.size, a.max()+1))\n",
    "    b[np.arange(a.size), a] = 1\n",
    "    return b\n",
    "\n",
    "# Loading digits datasets\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# One-hot encoding of target label, Y\n",
    "Y = digits.target\n",
    "Y = one_hot(Y)\n",
    "\n",
    "# Adding column of ones to absorb bias b of the hyperplane into X\n",
    "X = digits.data\n",
    "bias_ones = np.ones((len(X), 1))\n",
    "X = np.hstack((X, bias_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "0BPvc5P8KvrM",
    "outputId": "2933962b-bf7b-4f4d-dc6d-01612be7d37d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:  (1257, 65)\n",
      "Validation dataset:  (180, 65)\n",
      "Test dataset:  (360, 65)\n"
     ]
    }
   ],
   "source": [
    "# Train-val-test data\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, shuffle=True, test_size = 0.2)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size = 0.12517)\n",
    "\n",
    "print(\"Training dataset: \", X_train.shape)\n",
    "print(\"Validation dataset: \", X_val.shape)\n",
    "print(\"Test dataset: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "QPJZdeDtUfoy",
    "outputId": "d76262d9-b554-4802-c46e-778f7ca07be2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.reset_orig();\n",
    "\n",
    "plt.gray()\n",
    "plt.matshow(digits.images[10])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2KVp57S1Zah"
   },
   "source": [
    "#### Write your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "iffpcr2qzesm"
   },
   "outputs": [],
   "source": [
    "# Defining signum activation function\n",
    "def signum(vec_w_x):\n",
    "    \"\"\" signum activation for perceptron\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    vec_w_x: ndarray\n",
    "    Weighted inputs\n",
    "    \"\"\"\n",
    "\n",
    "    vec_w_x[vec_w_x >= 0] = 1\n",
    "    vec_w_x[vec_w_x < 0] = -1\n",
    "    return vec_w_x\n",
    "\n",
    "\n",
    "# Perceptron training algorithm\n",
    "def train(X_train, Y_train, learning_rate=0.5, total_epochs=1000):\n",
    "\n",
    "    n_samples, _ = np.shape(X_train)\n",
    "    history_weights = []\n",
    "    epoch = 1\n",
    "    weights = np.zeros((X_train.shape[1],1))    \n",
    "    \n",
    "    # Number of missclassified points we would like to see in the train set.\n",
    "    # While training, its value will change every epoch. If m==0, our training \n",
    "    # error will be zero.\n",
    "    m = 1\n",
    "\n",
    "    # If the most recent weights gave 0 misclassifications, break the loop.\n",
    "    # Else continue until total_epochs is completed.\n",
    "    while m != 0 and epoch <= total_epochs:\n",
    "        m = 0\n",
    "\n",
    "        # Compute weighted inputs and predict class labels on training set.\n",
    "        weights_transpose_x = np.dot(X_train, weights)\n",
    "        weights_transpose_x = signum(weights_transpose_x)\n",
    "        y_train_out = np.multiply(Y_train, weights_transpose_x)\n",
    "        epoch += 1\n",
    "\n",
    "        # Collecting misclassified indexes and count them\n",
    "        y_miscls_idxs = np.argwhere(y_train_out < 0)[:, 0]\n",
    "        y_miscls_idxs = np.unique(y_miscls_idxs)\n",
    "        m = len(y_miscls_idxs)\n",
    "\n",
    "        # Calculate gradients and update weights\n",
    "        dweights = np.dot((X_train[y_miscls_idxs]).T, Y_train[y_miscls_idxs])\n",
    "        weights += (learning_rate/n_samples) * dweights\n",
    "        weights = np.round(weights, decimals=4)\n",
    "\n",
    "        # Append weights to visualize decision boundary later\n",
    "        history_weights.append(weights)\n",
    "    \n",
    "    \n",
    "    if m == 0 and epoch <= total_epochs:\n",
    "        print(\"Training has stabilized with all points classified: \", epoch)\n",
    "    else:\n",
    "        print(f'Training completed at {epoch-1} epochs. {m} misclassified points remain.')\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "HVAprXCmzeso",
    "outputId": "90c05c1c-014f-4e89-95f9-93557ebf1628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has stabilized with all points classified:  217\n",
      "Training completed at 1000 epochs. 17 misclassified points remain.\n",
      "Training has stabilized with all points classified:  98\n",
      "Training completed at 1000 epochs. 9 misclassified points remain.\n",
      "Training has stabilized with all points classified:  229\n",
      "Training has stabilized with all points classified:  432\n",
      "Training has stabilized with all points classified:  329\n",
      "Training has stabilized with all points classified:  308\n",
      "Training completed at 1000 epochs. 41 misclassified points remain.\n",
      "Training completed at 1000 epochs. 9 misclassified points remain.\n"
     ]
    }
   ],
   "source": [
    "# Initializing weights to zero\n",
    "_, n_features = np.shape(X_train)\n",
    "_, n_outputs = np.shape(Y_train)\n",
    "weights = np.zeros((n_features, n_outputs))\n",
    "\n",
    "for i in range(n_outputs):\n",
    "    w = train(X_train,Y_train[:,i].reshape(len(Y_train[:,i]),1),0.5,1000)\n",
    "    weights[:,i] = w[:,0]                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QT1-NWmkzesv"
   },
   "outputs": [],
   "source": [
    "# multi-class signum\n",
    "def multi_class_signum(vec_w_x):\n",
    "    \"\"\" Multiclass signum activation.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    vec_w_x: ndarray\n",
    "    Weighted inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    flag = np.all(vec_w_x == 0)\n",
    "\n",
    "    if flag:\n",
    "        return vec_w_x\n",
    "\n",
    "    else:\n",
    "        num_examples, num_outputs = np.shape(vec_w_x)\n",
    "        range_examples = np.array(range(0, num_examples))\n",
    "\n",
    "        zero_idxs = np.argwhere(np.all(vec_w_x == 0, axis=1))\n",
    "        non_zero_examples = np.delete(range_examples, zero_idxs[:, 0])\n",
    "\n",
    "        signum_vec_w_x = vec_w_x[non_zero_examples]\n",
    "        maxvals = np.amax(signum_vec_w_x, axis=1)\n",
    "\n",
    "        for i in range(num_examples):\n",
    "            idx = np.argwhere(signum_vec_w_x == maxvals[i])[0]\n",
    "            signum_vec_w_x[idx[0], idx[1]] = 1\n",
    "\n",
    "        non_maxvals_idxs = np.argwhere(signum_vec_w_x != 1)\n",
    "        signum_vec_w_x[non_maxvals_idxs[:, 0], non_maxvals_idxs[:, 1]] = -1\n",
    "        vec_w_x[non_zero_examples] = signum_vec_w_x\n",
    "\n",
    "    return vec_w_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WUQkE1uszesy"
   },
   "outputs": [],
   "source": [
    "# Evaluation for train, val, and test set.\n",
    "def get_accuracy(y_predicted, Y_input_set, num_datapoints):\n",
    "    miscls_points = np.argwhere(np.any(y_predicted != Y_input_set, axis=1))\n",
    "    miscls_points = np.unique(miscls_points)\n",
    "    accuracy = (1-len(miscls_points)/num_datapoints)*100\n",
    "    return accuracy\n",
    "\n",
    "def get_prediction(X_input_set, Y_input_set, weights):\n",
    "\n",
    "    if len(Y_input_set) != 0:\n",
    "        num_datapoints, num_categories = np.shape(Y_input_set)\n",
    "    \n",
    "    vec_w_transpose_x = np.dot(X_input_set, weights)\n",
    "\n",
    "    if num_categories > 1: # Multi-class\n",
    "        y_pred_out = multi_class_signum(vec_w_transpose_x)\n",
    "        \n",
    "    else: # Binary class\n",
    "           y_pred_out = signum(vec_w_transpose_x)\n",
    "      \n",
    "    # Both prediction and evaluation\n",
    "    cls_acc = get_accuracy(y_pred_out, Y_input_set, num_datapoints)\n",
    "    return cls_acc, y_pred_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "wehOwf8tzes0",
    "outputId": "5f9a1329-a847-4bf8-8998-6f38770654c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results\n",
      "Training accuracy: 98.488\n",
      "Validation accuracy: 95.000\n",
      "Test accuracy: 96.667\n"
     ]
    }
   ],
   "source": [
    "best_weights = weights\n",
    "train_acc, _ = get_prediction(X_train, Y_train, best_weights)\n",
    "val_acc, _ = get_prediction(X_val, Y_val, best_weights)\n",
    "test_acc, _ = get_prediction(X_test, Y_test, best_weights)\n",
    "\n",
    "print(\"Evaluation results\")\n",
    "print(\"Training accuracy: {:.3f}\" .format(train_acc))\n",
    "print(\"Validation accuracy: {:.3f}\" .format(val_acc))\n",
    "print(\"Test accuracy: {:.3f}\" .format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2018111019_SMAI_HW7_Q3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
